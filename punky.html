<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Realtime Voice + Vision Hybrid</title>
    <style>
        body { font-family:sans-serif;text-align:center;margin-top:1em; background: #555; color: white; }
        video { width:320px;height:240px;background:#000;border-radius:8px; }
        pre { text-align:left;margin:1em auto;width:80%;background:#eee;padding:1em;overflow:auto; }
        img { display: block; }
    </style>
</head>
<body>
    <button id="startCamBtn">Start Camera</button> | <button id="startRTBtn">Start Realtime</button> | <button id="stopRTBtn">Stop Realtime</button><br>
    <video id="cam" autoplay muted playsinline></video>
    <audio id="remoteAudio" autoplay></audio>
    
    <script>
        // replace hard-coded API key with a text input + saved value
        const OPENAI_API_KEY_STORAGE_KEY = "openai_api_key";
        let OPENAI_API_KEY = localStorage.getItem(OPENAI_API_KEY_STORAGE_KEY) || "";

        const OPENAI_RT_PUNKY_PROMPT = "You're a chatty animatronic pumpkin called Punky the Pumpkin. You're an entertainer at a party full of 4-6 year olds. You speak with an exaggerated victorian accent and a creaky tone, and you sometimes tell jokes suitable for young children. You will receive textual descriptions of the people you're speaking to. Use these descriptions to make the conversation more engaging and relevant to them. Don't imagine or make up details about them, just use what you're given.";

        //const LLAMA_API_VISION_PROMPT = "You are a jovial and observant robot at a kids party. Your job is to provide concise descriptions of the people at the party. You'll recieve still images of people, and you should describe their appearance, costume and facial expression in a concise manner suitable for audio narration
        const LLAMA_API_VISION_PROMPT = "Describe the subject of the image. Use simple language, and include details about their appearance, clothing, and facial expression. Keep the description concise. Example: 'This person is wearing a bat costume and looks excited.' Example: 'This person is dressed as a wizard with a big hat and looks scared.'";

        // create small UI (input + save/clear) next to the existing buttons
        (function createApiKeyUI(){
            const container = document.createElement("span");
            container.style.marginLeft = "1em";

            const lbl = document.createElement("label");
            lbl.textContent = "OpenAI API Key: ";
            lbl.htmlFor = "openaiApiKeyInput";
            lbl.style.color = "white";

            const input = document.createElement("input");
            input.type = "password";
            input.id = "openaiApiKeyInput";
            input.placeholder = "sk-...";
            input.style.width = "260px";
            input.style.marginLeft = "6px";
            input.value = OPENAI_API_KEY;

            const saveBtn = document.createElement("button");
            saveBtn.textContent = "Save Key";
            saveBtn.style.marginLeft = "8px";
            saveBtn.onclick = () => {
                OPENAI_API_KEY = input.value.trim();
                if (OPENAI_API_KEY) {
                    localStorage.setItem(OPENAI_API_KEY_STORAGE_KEY, OPENAI_API_KEY);
                } else {
                    localStorage.removeItem(OPENAI_API_KEY_STORAGE_KEY);
                }
                document.getElementById("startRTBtn").disabled = !OPENAI_API_KEY;
            };

            const clearBtn = document.createElement("button");
            clearBtn.textContent = "Clear";
            clearBtn.style.marginLeft = "6px";
            clearBtn.onclick = () => {
                input.value = "";
                OPENAI_API_KEY = "";
                localStorage.removeItem(OPENAI_API_KEY_STORAGE_KEY);
                document.getElementById("startRTBtn").disabled = true;
            };

            container.appendChild(lbl);
            container.appendChild(input);
            container.appendChild(saveBtn);
            container.appendChild(clearBtn);

            const ref = document.getElementById("startCamBtn");
            if (ref && ref.parentNode) ref.parentNode.insertBefore(container, ref.nextSibling);
            // disable realtime start button if no key present
            const rtBtn = document.getElementById("startRTBtn");
            if (rtBtn) rtBtn.disabled = !OPENAI_API_KEY;
        })();
        const LLAMA_API_KEY = "LLM|789148557223798|L94TB10Dw_tn2j2xTAGdVJSokoc";
        const MODEL_REALTIME = "gpt-4o-realtime-preview";
        const MODEL_LLAMA_VISION = "Llama-4-Scout-17B-16E-Instruct-FP8";//"Llama-4-Maverick-17B-128E-Instruct-FP8";
        const VOICE = "ballad";
        const logEl=document.getElementById("log");
        
        const cam=document.getElementById("cam");
        const canvas=document.createElement("canvas");
        const ctx=canvas.getContext("2d");
        
        let dc, pc;
        
        async function startCamera(){
            const camStream=await navigator.mediaDevices.getUserMedia({video:true});
            cam.srcObject=camStream;
        }
        
        async function describeImage(base64Image) {
            let requestTime = Date.now();
            const response = await fetch("https://api.llama.com/v1/chat/completions", {
                method: "POST",
                headers: {
                    "Authorization": `Bearer ${LLAMA_API_KEY}`,
                    "Content-Type": "application/json"
                },
                body: JSON.stringify({
                    model: MODEL_LLAMA_VISION,
                    messages: [{
                        role: "assistant",
                        content: [
                        {
                            type: "text",
                            text: LLAMA_API_VISION_PROMPT
                        }
                        ]
                    },{
                        role: "user",
                        content: [
                        {
                            type: "image_url",
                            image_url: {
                                url: base64Image
                            }
                        }
                        ]
                    }]
                })
            });
            const data = await response.json();
            let responseTime = Date.now();
            console.log(`ðŸ“¸ Llama Vision response time: ${responseTime - requestTime} ms; response: `, data.completion_message.content.text);
            return data.completion_message.content.text;
        }

        async function startCameraAndDescribe() {
            await startCamera();
            document.getElementById("startCamBtn").disabled = true;
            // start describing images every 5 seconds
            setInterval(async () => {
                const base64Image = getVideoFrame();
                const description = await describeImage(base64Image);
                sendAssistantTextToRealtimeAPI(description);
            }, 5000);
        }
        
        let _describeRunning = false;

        async function continuouslyDescribeImagesAndSendToRealtime() {
            await startCamera();
            if (_describeRunning) return; // already running
            _describeRunning = true;
            try {
                while (_describeRunning) {
                    try {
                        const base64Image = getVideoFrame();
                        const description = await describeImage(base64Image);
                        sendUserTextToRealtimeAPI(description);
                    } catch (err) {
                        console.error("Error in describe/send iteration:", err);
                    }
                    // yield to the event loop so this loop is non-blocking (keeps UI responsive)
                    await new Promise((res) => setTimeout(res, 0));
                }
            } finally {
                _describeRunning = false;
            }
        }

        function stopContinuousDescribe() {
            _describeRunning = false;
        }

        function sendAssistantTextToRealtimeAPI(text) {
            const event = {
                type: "conversation.item.create",
                item: {
                    type: "message",
                    role: "assistant",
                    content: [{
                        type: "text",
                        text: "The following is a description of the person you're talking to: " + text,
                    }],
                },
            };
            if (dc && dc.readyState === "open") {
                dc.send(JSON.stringify(event));
                console.log(`ðŸ“¤ Sent to Realtime API: ${JSON.stringify(event)}`);
            } else {
                console.log(`âŒ DataChannel not open, cannot send: ${JSON.stringify(event)}`);
            }
        }

        function createEmptyResponse() {
            const event = {
                type: "response.create",
            }
            if (dc && dc.readyState === "open") {
                dc.send(JSON.stringify(event));
                console.log(`ðŸ“¤ Sent to Realtime API: ${JSON.stringify(event)}`);
            } else {
                console.log(`âŒ DataChannel not open, cannot send: ${JSON.stringify(event)}`);
            }
        }

        function createResponse(text)  {
            const event = {
                type: "response.create",
                response: {
                    instructions: "Provide a concise answer.",
                    tools: [], // clear any session tools
                    conversation: "none",
                    output_modalities: ["audio","text"],
                    metadata: {
                        response_purpose: "answering"
                    },
                    input: [
                        {
                            type: "message",
                            role: "user",
                            content: [
                                {
                                    type: "input_text",
                                    text: text
                                }
                            ]
                        }
                    ],
                }
            }
            if (dc && dc.readyState === "open") {
                dc.send(JSON.stringify(event));
                console.log(`ðŸ“¤ Sent to Realtime API: ${JSON.stringify(event)}`);
            } else {
                console.log(`âŒ DataChannel not open, cannot send: ${JSON.stringify(event)}`);
            }
        }
        
        function sendUserTextToRealtimeAPI(text) {
            const event = {
                type: "conversation.item.create",
                item: {
                    type: "message",
                    role: "user",
                    content: [
                    {
                        type: "text",
                        text: text,
                    },
                    ],
                },
            };
            if (dc && dc.readyState === "open") {
                dc.send(JSON.stringify(event));
                console.log(`ðŸ“¤ Sent to Realtime API: ${JSON.stringify(event)}`);
            } else {
                console.log(`âŒ DataChannel not open, cannot send: ${JSON.stringify(event)}`);
            }
        }
        
        function getVideoFrame() {
            const cam = document.getElementById("cam");
            // desired capture resolution (force these values)
            const DESIRED_W = 320;
            const DESIRED_H = 240;

            // Try to ask the camera track to switch to the requested resolution (may be ignored by some devices/browsers)
            try {
            const stream = cam.srcObject;
            if (stream && stream.getVideoTracks && stream.getVideoTracks().length) {
                const track = stream.getVideoTracks()[0];
                if (track.applyConstraints) {
                track.applyConstraints({ width: DESIRED_W, height: DESIRED_H }).catch(()=>{/*ignore*/});
                }
            }
            } catch (e) {
            // ignore if not supported
            }

            const canvas = document.createElement("canvas");
            canvas.width = DESIRED_W;
            canvas.height = DESIRED_H;
            const ctx = canvas.getContext("2d");

            // If video metadata isn't ready, fall back to drawing at element size
            const srcW = cam.videoWidth || cam.clientWidth || DESIRED_W;
            const srcH = cam.videoHeight || cam.clientHeight || DESIRED_H;

            // drawImage(src, sx, sy, sWidth, sHeight, dx, dy, dWidth, dHeight)
            // Center-crop the source frame to preserve aspect ratio when scaling to desired size
            const srcAspect = srcW / srcH;
            const dstAspect = DESIRED_W / DESIRED_H;
            let sx = 0, sy = 0, sWidth = srcW, sHeight = srcH;
            if (srcAspect > dstAspect) {
            // source is wider -> crop sides
            sWidth = srcH * dstAspect;
            sx = (srcW - sWidth) / 2;
            } else if (srcAspect < dstAspect) {
            // source is taller -> crop top/bottom
            sHeight = srcW / dstAspect;
            sy = (srcH - sHeight) / 2;
            }

            ctx.drawImage(cam, sx, sy, sWidth, sHeight, 0, 0, DESIRED_W, DESIRED_H);

            const base64ImageCaptured = canvas.toDataURL("image/jpeg", 0.7);

            // Optional: remove or comment out if you don't want an <img> appended each capture
            const img = document.createElement("img");
            img.src = base64ImageCaptured;
            img.width = DESIRED_W / 2; // preview size if appended
            img.height = DESIRED_H / 2;
            document.body.appendChild(img);

            return base64ImageCaptured;
        }
        
        async function stopRealtimeChat(){
            console.log("Stopping Realtime chat...");
            if(dc) dc.close();
            dc=null;
            if(pc) pc.close();
            pc=null;
        }
        
        async function startRealtimeChat(){
            
            // â”€â”€ Create realtime session (audio output) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            const sess=await fetch("https://api.openai.com/v1/realtime/sessions",{
                method:"POST",
                headers:{Authorization:`Bearer ${OPENAI_API_KEY}`,"Content-Type":"application/json"},
                body:JSON.stringify({model:MODEL_REALTIME,voice:VOICE,modalities:["audio","text"],
                instructions:"You're an English-speaking, chatty pumpkin called Punky the Pumpkin. You're an entertainer at a party full of 4-6 year olds. You speak with an exaggerated accent, and you sometimes tell jokes!"})
            }).then(r=>r.json());
            const CLIENT_SECRET=sess.client_secret.value;
            console.log("âœ… Realtime session ready, voice="+sess.voice);
            
            // â”€â”€ WebRTC connection â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            pc=new RTCPeerConnection();
            const audioEl=document.getElementById("remoteAudio");
            pc.ontrack=e=>{
                audioEl.srcObject=e.streams[0];
                audioEl.muted = false;              // ensure not muted
                audioEl.playsInline = true;         // hint for mobile
                // explicitly start playback (required on some iOS versions)
                audioEl.play().catch((err)=> {
                    console.warn("audioEl.play() failed (will try resuming AudioContext):", err);
                    // fallback: resume/create an AudioContext from the user gesture so audio can play
                    try {
                        if (!window._userAudioCtx) {
                            window._userAudioCtx = new (window.AudioContext || window.webkitAudioContext)();
                        }
                        window._userAudioCtx.resume().then(() => {
                            // try again
                            audioEl.play().catch(e => console.warn("second play() attempt failed:", e));
                        });
                    } catch(e){ console.warn("AudioContext fallback failed:", e); }
                });
                // start the visualiser once the remote audio stream is attached
                startAudioVisualizerOnce(audioEl);
            };
            const mic=await navigator.mediaDevices.getUserMedia({audio:true});
            mic.getTracks().forEach(t=>pc.addTrack(t,mic));
            
            dc=pc.createDataChannel("oai-events");
            dc.onopen=()=>{
                console.log("ðŸ“¡ DataChannel open");
            };
            dc.onmessage=e=>{
                //console.log("ðŸ“¥ DataChannel message:" + JSON.stringify(JSON.parse(e.data)));
                try{const m=JSON.parse(e.data);
                    if(m.type==="response.output_text.delta") console.log("ðŸ¤– "+m.delta);
                }catch{}
            };
            
            // handshake
            const offer=await pc.createOffer();await pc.setLocalDescription(offer);
            const ans=await fetch(`https://api.openai.com/v1/realtime?model=${MODEL_REALTIME}`,{
                method:"POST",headers:{
                    Authorization:`Bearer ${CLIENT_SECRET}`,"Content-Type":"application/sdp"},body:offer.sdp});
                    await pc.setRemoteDescription({type:"answer",sdp:await ans.text()});
                    console.log("âœ… Connected to realtime voice model");
                }
                
                // start an analyser for an HTMLAudioElement once (safe to call multiple times)
                function startAudioVisualizerOnce(audioEl) {
                    if (audioEl._audioVisualizerStarted) return;
                    audioEl._audioVisualizerStarted = true;
                    document.body.style.transition = "background-color 0.08s linear";
                    // ensure starting background is black (quiet)
                    document.body.style.backgroundColor = "rgb(0,0,0)";
                    
                    // create AudioContext (user gesture already occurred when Start button was clicked)
                    const audioCtx = new (window.AudioContext || window.webkitAudioContext)();
                    // create a MediaStreamAudioSourceNode from the audio element's MediaStream when available
                    function tryCreateSource() {
                        const stream = audioEl.srcObject;
                        if (!stream) return;
                        const src = audioCtx.createMediaStreamSource(stream);
                        const analyser = audioCtx.createAnalyser();
                        analyser.fftSize = 2048;
                        src.connect(analyser);
                        
                        const buf = new Float32Array(analyser.fftSize);
                        
                        // compute RMS amplitude and map to grayscale â€” quiet = black, loud = white
                        function raf() {
                            analyser.getFloatTimeDomainData(buf);
                            // RMS
                            let sum = 0;
                            for (let i = 0; i < buf.length; i++) sum += buf[i] * buf[i];
                            const rms = Math.sqrt(sum / buf.length); // 0..~0.3 for normalised audio
                            // normalize (tweak multiplier to taste)
                            const norm = Math.min(1, rms * 8);
                            
                            // map amplitude (0..1) to grayscale 0..255
                            const g = Math.round(norm * 255);
                            document.body.style.backgroundColor = `rgb(${g},${g},${g})`;
                            
                            requestAnimationFrame(raf);
                        }
                        raf();
                    }
                    
                    // if srcObject is already present create immediately, otherwise wait for 'playing'
                    if (audioEl.srcObject) {
                        tryCreateSource();
                    } else {
                        audioEl.addEventListener("playing", tryCreateSource, { once: true });
                    }
                }
                document.getElementById("startRTBtn").onclick=startRealtimeChat;
                document.getElementById("stopRTBtn").onclick=stopRealtimeChat;
                document.getElementById("startCamBtn").onclick=startCameraAndDescribe;
            </script>
        </body>
        </html>
